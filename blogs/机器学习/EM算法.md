---
title: EM方法
date: 2023/11/04 19:57:00
categories:
    - 机器学习
tags: 
    - 统计
---

EM（Expectation-Maximum）算法也称期望最大化算法，曾入选“数据挖掘十大算法”中，可见EM算法在机器学习、数据挖掘中的影响力。EM算法是最常见的隐变量估计方法，在机器学习中有极为广泛的用途，例如常被用来学习高斯混合模型（Gaussian mixture model，简称GMM）的参数；隐式马尔科夫算法（HMM）、LDA主题模型的变分推断等等。

## **EM算法简介**

EM算法是一种迭代优化策略，由于它的计算方法中每一次迭代都分两步，其中一个为期望步（E步），另一个为极大步（M步），所以算法被称为EM算法（Expectation-Maximization  Algorithm）。EM算法受到缺失思想影响，最初是为了解决**数据缺失情况下的参数估计**问题，其基本思想是：首先根据已经给出的观测数据，估计出模型参数的值；然后再依据上一步估计出的参数值估计缺失数据的值，再根据估计出的缺失数据加上之前已经观测到的数据重新再对参数值进行估计，然后反复迭代，直至最后收敛，迭代结束。



## **预备知识**

想清晰的了解EM算法推导过程和其原理，我们需要知道两个基础知识：“极大似然估计”和“Jensen不等式”。

### 极大似然估计

假如我们需要调查学校的男生和女生的身高分布 ，我们抽取100个男生和100个女生，将他们按照性别划分为两组。然后，统计抽样得到100个男生的身高数据和100个女生的身高数据。如果我们知道他们的身高服从正态分布，但是这个分布的均值 $\mu$ 和方差 $\delta^2$ 是不知道，这两个参数就是我们需要估计的。

![图片.png](https://s2.loli.net/2023/11/04/C8TiEmUvycPSpF2.png)

我们已知的条件有两个：样本服从的分布模型、随机抽取的样本。我们需要求解模型的参数。根据已知条件，通过极大似然估计，求出未知参数。总的来说：<mark>极大似然估计就是用来估计模型参数的统计学方法</mark>。

（2）用数学知识解决现实问题

问题数学化：样本集 $X=\{x_1,x_2,\dots,x_N \}, N=100$。概率密度是：$p(x_i|\theta)$ 抽到第 $i$ 个男生身高的概率。由于100个样本之间独立同分布，所以同时抽到这100个男生的概率是它们各自概率的乘积，就是从分布是 $p(X|\theta)$的总体样本中抽取到这100个样本的概率，也就是样本集$X$中各个样本的联合概率，用下式表示：
$$
L(\theta)=L(x_1,x_2, \dots, x_n; \theta) = \prod_{i=1}^{n}p(x_i;\theta),\theta \in \Theta
$$
这个概率反映了在概率密度函数的参数是 $\theta$ 时，得到 $X$ 这组样本的概率。 我们需要找到一个参数 $\theta$ ，使得抽到 $X$ 这组样本的概率最大，也就是说需要其对应的似然函数 $L(\theta)$ 最大。满足条件的 $\theta$ 叫做 $\theta$ 的最大似然估计值，记为：
$$
\hat{\theta}=argmaxL(\theta)
$$
（3）最大似然函数估计值的求解步骤

- 首先，写出似然函数

$$
L(\theta) = L(x_1,x_2,\dots,x_n;\theta)=\prod_{i=1}^{n}p(x_i;\theta),\theta\in\Theta
$$

- 其次，对似然函数取对数

$$
l(\theta)=lnL(\theta)=ln\prod_{i=1}^{n}p(x_i;\theta)=\sum_{i=1}^{n}lnp(x_i;\theta)
$$

- 然后，对上式求导，另导数为0，得到似然方程。
- 最后，解似然方程，得到的参数值即为所求。

多数情况下，我们是根据已知条件来推算结果，而极大似然估计是已知结果，寻求使该结果出现的可能性最大的条件，以此作为估计值。



### Jensen不等式

（1）定义

设 $f$ 是定义域为实数的函数，如果对所有的实数 $x$，$f(x)$ 的二阶导数都大于0，那么 $f$ 是凸函数。

Jensen不等式定义如下：

如果 $f$ 是凸函数，$X$ 是随机变量，那么： $E[f(X)] \geq f(E[X])$。当且仅当 $X$ 是常量时，该式取等号。其中，$E(X)$ 表示 $X$ 的数学期望。

注：Jensen不等式应用于凹函数时，不等号方向反向。当且仅当x是常量时，该不等式取等号。



## **EM算法详解**

### 问题描述

我们目前有100个男生和100个女生的身高，但是我们不知道这200个数据中哪个是男生的身高，哪个是女生的身高，即抽取得到的每个样本都不知道是从哪个分布中抽取的。这个时候，对于每个样本，就有两个未知量需要估计：

（1）这个身高数据是来自于男生数据集合还是来自于女生？

（2）男生、女生身高数据集的正态分布的参数分别是多少？

![图片.png](https://s2.loli.net/2023/11/04/C8TiEmUvycPSpF2.png)

那么，对于具体的身高问题使用EM算法求解步骤

[![piQYed1.png](https://z1.ax1x.com/2023/11/05/piQYed1.png)](https://imgse.com/i/piQYed1)

（1）初始化参数：先初始化男生身高的正态分布的参数：如均值=1.65，方差=0.15

（2）计算每一个人更可能属于男生分布或者女生分布；

（3）通过分为男生的n个人来重新估计男生身高分布的参数（最大似然估计），女生分布也按照相同的方式估计出来，更新分布。

（4）这时候两个分布的概率也变了，然后重复步骤（1）至（3），直到参数不发生变化为止。



### EM算法推导流程

对于n个样本观察数据 $x=\{x_1,x_2,\dots,x_n \}$，找出样本的模型参数 $\theta$ ，极大化模型分布的对数似然函数如下：
$$
\hat{\theta}=argmax\sum_{i=1}^nlogp(x_i;\theta)
$$
如果我们得到的观察数据有未观察到的隐含数据 $z=(z_1,z_2,\dots,z_n)$  ，即上文中每个样本属于哪个分布是未知的，此时我们极大化模型分布的对数似然函数如下：
$$
\hat{\theta}=argmax\sum_{i=1}^nlogp(x_i;\theta)=argmax\sum_{i=1}^{n}log\sum_{z_i}p(x_i,z;\theta)
$$
上面这个式子是根据 $x_i$ 的边缘概率计算得来，没有办法直接求出 $\theta$ 。因此需要一些特殊的技巧，使用Jensen不等式对这个式子进行缩放如下：
$$
\sum_{i=1}^{n}log\sum_{z_i}p(x_i,z;\theta)=\sum_{i=1}^{n}log\sum_{z_i}Q_i(z_i)\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}
$$

$$
\sum_{i=1}^{n}log\sum_{z_i}p(x_i,z;\theta) \geq \sum_{i=1}^{n}\sum_{z_i}Q_i(z_i)log\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}
$$

(1)式是引入了一个未知的新的分布 $Q_i(z_i)$ ，分子分母同时乘以它得到的。

(2)式是由(1)式根据Jensen不等式得到的。由于 $\sum_{z_i}Q_i(z_i)[log\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}]$ 为 $\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}$ 的期望，且$log(x)$为凹函数，根据Jensen不等式可由(1)式得到(2)式。



上述过程可以看作是对 $logl(\theta)$ 求了下界$(l(\theta)=\sum_{i=1}^nlogp(x_i;\theta))$。对于 $Q_i(z_i)$ 我们如何选择呢？假设 $\theta$ 已经给定，那么 $logl(\theta)$ 的值取决于 $Q_i(z_i)$ 和 $p(x_i;z_i)$ 。我们可以通过调整这两个概率使(2)式下界不断上升，来逼近 $logl(\theta)$ 的真实值。那么如何算是调整好呢？当不等式变成等式时，说明我们调整后的概率能够等价于 $logl(\theta)$ 了。按照这个思路，我们要找到等式成立的条件。



如果要满足Jensen不等式的等号，则有：

$\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}=c,\text{c为常数}$

由于 Q_i(z_i) 是一个分布，所以满足：$\sum_zQ_i(z_i)=1$，则 $\sum_zp(x_i,z_i;\theta)=c$。

由上面两个式子，我们可以得到：

$Q_i(z_i)=\frac{p(x_i,z_i;\theta)}{\sum_zp(x_i,z_i;\theta)}=\frac{p(x_i,z_i;\theta)}{p(x_i;\theta)}=p(z_i|x_i;\theta)$

至此，我们推出了在固定其他参数 $\theta$ 后，$Q_i(z_i)$ 的计算公式就是后验概率，解决了$Q_i(z_i)$ 如何选择的问题。

如果$Q_i(z_i)=p(z_i|x_i;\theta)$，则(2)式是我们包含隐藏数据的对数似然函数的一个下界。如果我们能最大化(2)式这个下界，则也是在极大化我们的对数似然函数。即我们需要最大化下式：
$$
argmax\sum_{i+1}^{n}\sum_{z_i}Q_i(z_i) log\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}
$$
上式也就是我们的EM算法的M步，那E步呢？解决了 $Q_i(z_i)$ 如何选择的问题，这一步就是E步，该步建立了 $l(\theta)$ 的下界。

### EM算法流程

**输入：**观察到的数据 $x=(x_1,x_2,\dots,x_n)$ ，联合分布 $p(x,z;\theta)$ ，条件分布 $p(z|x,\theta)$ ，最大迭代次数 $J$ 。

**算法步骤：**

（1）随机初始化模型参数 $\theta$ 的初值 $\theta_0$ 。

（2）$j=1,2,\dots,J$ 开始EM算法迭代：

- E步：计算联合分布的条件概率期望：

$$
Q_i(z_i)=p(z_i|x_i,\theta_j)
$$

$$
l(\theta,\theta_j)=\sum_{i=1}^n\sum_{z_i}Q_i(z_i)log\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}
$$

- M步：极大化 $l(\theta,\theta_j)$，得到 $\theta_{j+1}$

$$
\theta_{j+1}=argmaxl(\theta,\theta_j)
$$

- 如果 $\theta_{j+1}$ 已经收敛，则算法结束

输出：模型参数 $\theta$



## EM算法的收敛性

- EM算法是否一定收敛？

结论：**EM算法可以保证收敛到一个稳定点，即EM算法是一定收敛的**。



- 如果EM算法收敛，能否保证收敛到全局最大值？

结论：EM算法可以保证收敛到一个稳定点，但是却<mark>不能保证收敛到全局的极大值点</mark>，因此它是局部最优的算法，当然，如果我们的优化目标 $l(\theta, \theta_l)$ 是凸的，则EM算法可以保证收敛到全局最大值，这点和梯度下降法这样的迭代算法相同。
