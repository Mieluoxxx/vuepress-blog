---
title: 聚类方法
date: 2023/11/03 16:17:00
categories:
    - 机器学习
tags: 
    - 聚类
---

## **聚类与分类的区别**

​	分类：类别是已知的，通过对已知类别的数据进行训练和学习，找到这些不同类的特征，再对未知类别的数据进行分类，属于**监督学习**。

​	聚类：事先不知道数据会分为几类，通过聚类分析，将数据聚合成几个群体。聚类不需要对数据进行训练和学习，属于**无监督学习**。

## 聚类的基本概念

聚类的核心概念是相似度（similarity）或距离（distance），有多种相似度或距离的定义。因为相似度直接影响聚类的结果，所以其选择是聚类的根本问题。具体哪种相似度更合适取决于应用问题的特性。

### 闵可夫斯基距离

**闵可夫斯基距离** (Minkowski Distance)，也被称为 **闵氏距离**。它不仅仅是一种距离，而是<mark>将多个距离公式（曼哈顿距离、欧式距离、切比雪夫距离）**总结成为的一个公式**</mark>。
$$
d_{ij} = \sqrt[p]{\sum^{n}_{k=1}\mid X_{ik}-X_{jk} \mid ^p}
$$
|     p      |     对应     |
| :--------: | :----------: |
|    p=1     |  曼哈顿距离  |
|    p=2     |   欧氏距离   |
| p=$\infty$ | 切比雪夫距离 |



### 马哈拉诺比斯距离

**马哈拉诺比斯距离**（Mahalanobis distance），简称马氏距离，是欧式距离的推广(当各个分量独立)，<mark>考虑**各个分量（特征）之间的相关性**并与各个分量的尺度无关，表示数据的**协方差距离**</mark>。马哈拉诺比斯距离越大相似度越小，距离越小相似度越大。
$$
d_{ij}=[(X_i-X_j)^TS^{-1}(X_i-X_j)]^{1/2}
$$

$$
S=\frac{1}{n}(X-\mu_X)(X-\mu_X)^T
$$



### 相关系数

样本之间的相似度也可以用相关系数（correlation coefficient）来表示。相关系数的绝对值越接近于1，表示样本越相似；越接近于0，表示样本越不相似。
$$
r_{ij} = \frac{\sum^{m}_{k=1}(x_{ki}-\bar{x}_i)(x_{kj}-\bar{x}_j)}{[\sum^{m}_{k=1}(x_{ki}-\bar{x}_i)^2 \sum^{m}_{k=1}(x_{kj}-\bar{x}_j)^2]^\frac{1}{2}}
$$



### 夹角余弦

样本之间的相似度也可以用夹角余弦（cosine）来表示。夹角余弦越接近于1，表示样本越相似；越接近于0，表示样本越不相似。
$$
S_{ij}=\frac{\sum_{k=1}^{m}x_{ki}x_{kj}}{[\sum_{k=1}^{m}x_{ki}^2\sum_{k=1}^{m}x_{kj}^2]^\frac{1}{2}}
$$


## 聚类方法

本章介绍两种最常用的聚类算法：层次聚类（hierarchical clustering）和k均值聚类（k-means clustering）

- 层次聚类又有聚合（自下而上）和分裂（自上而下）两种方法。
- 聚合法开始将每个样本各自分到一个类；之后将相距最近的两类合并，建立一个新的类，重复此操作直到满足停止条件；得到层次化的类别。
- 分裂法开始将所有样本分到一个类；之后将已有类中相距最远的样本分到两个新的类，重复此操作直到满足停止条件；得到层次化的类别。
- k均值聚类是基于中心的聚类方法，通过迭代，将样本分到k个类中，使得每个样本与其所属类的中心或均值最近；得到k个“平坦的”、非层次化的类别，构成对空间的划分。

### 层次聚类

聚合聚类的具体过程如下：对于给定的样本集合，开始将每个样本分到一个类；然后按照一定规则，例如类间距离最小，将最满足规则条件的两个类进行合并；如此反复进行，每次减少一个类，直到满足停止条件，如所有样本聚为一类。

#### 算法设计

输入：n个样本组成的样本集合及样本之间的距离；

输出：对样本集合的一个层次化聚类。

（1）计算n个样本两两之间的欧氏距离$d_{ij}$，记作矩阵$D=[d_{ij}]_{n×n}$。

（2）构造n个类，每个类只包含一个样本。

（3）合并类间距离最小的两个类，其中最短距离为类间距离，构建一个新类。

（4）计算新类与当前各类的距离。若类的个数为1，终止计算，否则回到步（3）。可以看出聚合层次聚类算法的复杂度是O（$n^3m$），其中m是样本的维数，n是样本个数。

### k均值聚类

​	聚类算法有很多种，K-Means 是聚类算法中的最常用的一种，算法最大的特点是简单，好理解，运算速度快，但是只能应用于<mark>**连续型**</mark>的数据，并且一定要在聚类前需要手工指定要分成几类。

K-Means 聚类算法的大致思想就是“物以类聚，人以群分”：

1. 首先输入 k 的值，即我们指定希望通过聚类得到 k 个分组；
2. 从数据集中随机选取 k 个数据点作为初始质心；
3. 对集合中每一个点，计算与每一个质心的距离，离哪个质心距离近，就属于哪一个质心的组；
4. 这时每一个质心都聚集了一堆点，这时候通过算法选出新的质心；
5. 如果新质心和旧质心之间的距离小于某一个设置的阈值（表示重新计算的质心的位置变化不大，趋于稳定，或者说收敛），可以认为我们进行的聚类已经达到期望的结果，算法终止；
6. 如果新质心和旧质心距离变化很大，需要迭代3~5步骤。

k均值聚类的策略是通过损失函数的最小化选取最优的划分或函数$C^∗$。首先，采用欧氏距离平方（squared Euclidean distance）作为样本之间的距离$d(x_i,x_j)$
$$
d(x_i,x_j)=\sum_{k=1}^{m}(x_{ki}-x_{kj})^2=||x_i-x_j||^2
$$
然后，定义样本与其所属类的中心之间的距离的总和为损失函数，即
$$
W(C)=\sum_{l=1}^{k}\sum_{C(i)=l}||x_i-\bar{x}_{l}||^2
$$
k均值聚类就是求解最优化问题：
$$
C^∗=arg \underset{c}{min}W(c)
$$
相似的样本被聚到同类时，损失函数值最小，这个目标函数的最优化能达到聚类的目的。

#### 算法设计

输入：n个样本的集合X；

输出：样本集合的聚类C。

（1）初始化。令t=0，随机选择k个样本点作为初始聚类中心$m^{(0)}=(m^{(0)}_1,m^{(0)}_2, \dots , m^{(0)}_k)$。

（2）对样本进行聚类。对固定的类中心$m^{(t)}=(m^{(t)}_1,m^{(t)}_2, \dots , m^{(t)}_k, \dots ,m^{(t)}_k)$，其中 $m^{(t)}_l$ 为类$G_l$的中心，计算每个样本到类中心的距离，将每个样本指派到与其最近的中心的类中，构成聚类结果C(t)。

（3）计算新的类中心。对聚类结果$C^{(t)}$，计算当前各个类中的样本的均值，作为新的类中心$m^{(t+1)}=(m^{(t+1)}_1,m^{(t+1)}_2, \dots , m^{(t+1)}_k, \dots ,m^{(t+1)}_k)$。

（4）如果迭代收敛或符合停止条件，输出 $C^∗=C(t)$。否则，令 $t=t+1$，返回步（2）。k均值聚类算法的复杂度是O（mnk），其中m是样本维数，n是样本个数，k是类别个数。



## 聚类质量的评估

### 内部指标

内部评估指标主要基于数据集的集合结构信息从紧致性、分离性、连通性和重叠度等方面对聚类划分进行评价。即基于数据聚类自身进行评估的。

#### 轮廓系数

​	轮廓系数适用于实际类别信息未知的情况。旨在将<mark>**某个对象与自己的簇的相似程度和与其他簇的相似程度**</mark>作比较。轮廓系数的取值范围是[-1,1]，同类别样本距离越相近，不同类别样本距离越远，值越大。当值为负数时，说明聚类效果很差。

- 对于单个样本，设a是与它同类别中其他样本的平均距离，b是与它距离最近不同类别中样本的平均距离，其轮廓系数为：

$$
s=\frac{b-a}{max(a,b)}
$$

- 对于一个样本集合，它的轮廓系数是所有样本轮廓系数的平均值。

### 外部指标

外部有效指标是指**当数据集的外部信息可用时**，通过比较聚类划分与外部准则的匹配度，可以评价不同聚类算法的性能。即<mark>通过将聚类结果与已经有“ground truth”分类进行对比</mark>。

不过该方法是有问题的，如果真的有了label，那么还需要聚类干嘛，而且实际应用中，往往都没label；另一方面，这些label只反映了数据集的一个可能的划分方法，它并不能告诉你存在一个不同的更好的聚类算法。

#### 标准化互信息

互信息是用来衡量两个数据分布的吻合程度。它也是一有用的信息度量，它是指两个事件集合之间的相关性。互信息越大，词条和类别的相关程度也越大。

假设U与V是对N个样本标签的分配情况，则两种分布的熵（熵表示的是不确定程度）分别为：
$$
H(U)=−\sum_{i=1}^{∣U∣}P(i)log⁡(P(i))
$$

$$
H(V) = - \sum_{j=1}^{|V|}P'(j)\log(P'(j))
$$

U与V之间的互信息的表达式为：
$$
\text{MI}(U, V) = \sum_{i=1}^{|U|}\sum_{j=1}^{|V|}P(i, j)\log\left(\frac{P(i,j)}{P(i)P'(j)}\right)
$$
其中，$P(i, j) = |U_i \cap V_j| / N$ 是随机选取的对象同时属于 $U_i$ 类和 $V_j$ 类的概率。

它也可以用集合基数公式表示：
$$
\text{MI}(U, V) = \sum_{i=1}^{|U|} \sum_{j=1}^{|V|} \frac{|U_i \cap V_j|}{N}\log\left(\frac{N|U_i \cap V_j|}{|U_i||V_j|}\right)
$$
标准互信息的表达式为：
$$
\text{NMI}(U, V) = \frac{\text{MI}(U, V)}{\text{mean}(H(U), H(V))}
$$
利用基于互信息的方法来衡量聚类效果需要实际类别信息，MI与NMI取值范围为[0,1]，它们都是值越大意味着聚类结果与真实情况越吻合。



示例：

$gnd = [\underbrace{111111}_{\text{6}}, \underbrace{222222}_{\text{6}}, \underbrace{33333}_{\text{5}}]$

$grps=[12\underbrace{1111}_{\text{4}}, 1\underbrace{2222}_{\text{4}}3,11\underbrace{333}_{\text{3}}]$

gnd 是 ground truth 的意思，grps 表示聚类后的 groups. 问题：计算序列 gnd 和 grps 的 NMI.

**先计算联合概率分布$p(grap,gnd)$**

| grap↓gnd ↑ |           1           |           2           |           3           |
| :--------: | :-------------------: | :-------------------: | :-------------------: |
|     1      | $p(1,1)=\frac{5}{17}$ | $p(1,2)=\frac{1}{17}$ | $p(1,3)=\frac{2}{17}$ |
|     2      | $p(2,1)=\frac{1}{17}$ | $p(2,2)=\frac{4}{17}$ | $p(2,3)=\frac{0}{17}$ |
|     3      | $p(3,1)=\frac{0}{17}$ | $p(3,2)=\frac{1}{17}$ | $p(3,3)=\frac{3}{17}$ |

**计算边际分布**

$P(gnd)=(\frac{6}{17},\frac{6}{17},\frac{5}{17})$

$P(grps)=(\frac{8}{17},\frac{5}{17},\frac{4}{17})$

**计算熵和互信息**

$H(gnd) = 1.58$

$H(grps) = 1.522$

$H(gnd|grps) = 1.014$

$I(gnd;grps) = H(gnd)-H(gnd|grps)=0.564$

**计算 NMI**

$\text{NMI} = \frac{2\times I(gnd;grps)}{H(gnd)+H(grps)} \approx 0.3649$

```python
def NMI(result, label):
    # 标准化互信息
    total_num = len(label)
    cluster_counter = collections.Counter(result)
    original_counter = collections.Counter(label)
    
    # 计算互信息量
    MI = 0
    eps = 1.4e-45 # 取一个很小的值来避免log 0
    
    for k in cluster_counter:
        for j in original_counter:
            count = 0
            for i in range(len(result)):
                if result[i] == k and label[i] == j:
                    count += 1
            p_k = 1.0*cluster_counter[k] / total_num
            p_j = 1.0*original_counter[j] / total_num
            p_kj = 1.0*count / total_num
            MI += p_kj * math.log(p_kj /(p_k * p_j) + eps, 2)
    
    # 标准化互信息量
    H_k = 0
    for k in cluster_counter:
        H_k -= (1.0*cluster_counter[k] / total_num) * math.log(1.0*cluster_counter[k] / total_num+eps, 2)
    H_j = 0
    for j in original_counter:
        H_j -= (1.0*original_counter[j] / total_num) * math.log(1.0*original_counter[j] / total_num+eps, 2)
        
    return 2.0 * MI / (H_k + H_j)
```



## 优化方向：KMeans++

Kmeans++的方法则是针对迭代次数。我们通过某种方法**降低收敛需要的迭代次数，从而达到快速收敛的目的**。

首先，如果我们<mark>**随机选择k个样本点作为起始的簇中心效果比随机k个坐标点更好**</mark>。原因也很简单，因为我们随机坐标对应的是在最大和最小值框成的矩形面积当中选择K个点，而我们从样本当中选K个点的范围则要小得多。我们可以单纯从面积的占比就可以看得出来。由于样本具有聚集性，我们在样本当中选择起始状态，选到接近类簇的可能性要比随机选大得多。

同时我们可以发现，**簇是有向心性的**。也就是说在同一个簇附近的点都会被纳入这个簇的范围内，反过来说就是两个离得远的点属于不同簇的可能性比离得近的大。

### 算法原理

1. 首先，其实的簇中心是我们通过在样本当中随机得到的。不过我们并不是一次性随机k个，而是只随机1个。

2. 接着，我们要从剩下的n-1个点当中再随机出一个点来做下一个簇中心。但是我们的随机不是盲目的，我们希望设计一个机制，**使得距离所有簇中心越远的点被选中的概率越大，离得越近被随机到的概率越小**。

3. 我们重复上述的过程，直到一共选出了K个簇中心为止。

### 轮盘法

我们来看一下如何根据权重来确定概率，实现这点的算法有很多，其中比较简单的是轮盘法。对于每一个点被选中的概率是：
$$
P(x_i)=\frac{f(x_i)}{\sum_{j=1}^{n}f(x_i)}
$$
其中 $f(x_i)$ 是每个点到所有类簇的最短距离， $P(x_i)$ 表示点 $x_i$ 被选中作为类簇中心的概率。

同样，我们通过实验来证明，首先我们来写出代码。我们需要一个辅助函数用来**计算某个样本和已经选好的簇中心之间的最小距离**，我们要用这个距离来做轮盘算法。

```python
def get_cloest_dist(point, centroids):    # 首先赋值成无穷大，依次递减
	min_dist = math.inf    
  for centroid in centroids:        
    dist = calculateDistance(point, centroid)
    if dist < min_dist:            
      min_dist = dist    
  return min_dist
```

接着就是用轮盘法选出K个中心，首先我们先随机选一个，然后再根据距离这个中心的举例用轮盘法选下一个，依次类推，直到选满K个中心为止。

```python
def kmeans_plus(dataset, k):    
  clusters = []    
  n = dataset.shape[0]    # 首先先选出一个中心点    
  rdx = np.random.choice(range(n), 1)    # np.squeeze去除多余的括号    
  clusters.append(np.squeeze(dataset[rdx]).tolist())    
  d = [0 for _ in range(len(dataset))]    
  for _ in range(1, k):        
    tot = 0        # 计算当前样本到已有簇中心的最小距离        
    for i, point in enumerate(dataset):            
      d[i] = get_cloest_dist(point, clusters)            
      tot += d[i]        
      # random.random()返回一个0-1之间的小数        
      # 总数乘上它就表示我们随机转了轮盘        
      tot *= random.random()        # 轮盘法选择下一个簇中心        
      for i, di in enumerate(d):            
        tot -= di            
        if tot > 0:                
          continue            
        clusters.append(np.squeeze(dataset[i]).tolist())            
        break    
  return np.mat(clusters)

```



## K-Means算法和EM算法的关系

K-means 是EM算法的特例，两者同样是随机赋值，反复对照，不断逼近。

- K-means的目标是选取n个中心点，将数据分成n类
- EM算法找到分类的规律，在聚类的同时找到更多
