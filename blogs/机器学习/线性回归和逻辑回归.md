---
title: 线性回归和逻辑回归
date: 2023/11/06
categories:
    - 机器学习
tags: 
    - 模型
---

在当今高手云集的传统机器学习界，虽然集成学习模型（Boosting，Bagging，Stacking）以左手降龙十八掌，右手乾坤大挪移打遍天下无敌手╰（‵□′）╯。但是作为其中最为经典的算法之一，线性回归（Linear Regression），是所有机器学习初学者的起点。逻辑回归（Logistic  Regression）是线性回归的一个推广，逻辑回归也可以说是最为经典的分类算法之一。相对其他的机器学习算法来说，逻辑回归比较容易理解（深究还是挺不简单的），适合初学者上手。

*预告：本文从线性回归推广到广义线性回归，再引入逻辑回归，中间会夹杂一些简单公式，不影响阅读噢* o(*￣▽￣*)ブ

## **引入**

在进入正题之前，先讲一个可能很多人（包括非初学者）没有注意到的问题：逻辑回归为什么叫“逻辑”？既然是分类算法，为什么又叫“回归”？

其实逻辑回归和逻辑二字没有实际上的关系，纯粹是由“Logistic”音译而来，那么Logistic又该怎么解释呢？且Regression的确是回归的意思，那又该如何解释呢？

这两个问题的答案最后再揭开（不是我不想说_(:з)∠)_，是因为涉及到底层原理....）

## **线性回归**

在介绍逻辑回归之前，先用线性回归来热热身。线性回归几乎是最简单的模型了，它假设因变量和自变量之间是线性关系的，一条直线简单明了。

在有监督（有标签的）学习中，我们会有一份数据集，由一列观测（ $y$ ，即因变量）和多列特征（ $X$ ，即自变量）组成。线性回归的目的就是<mark>找到和样本拟合程度最佳的线性模型</mark>，在寻找过程中需要确定系数 $\beta$ 和干扰项 $\varepsilon$（干扰项的作用是捕获除了X之外所有影响y的其他因素）。

直接上公式吧，有“看公式会发困病”的同学可以直接跳过到 ***加粗斜体黑字\***  噢 (●ˇ∀ˇ●)~：

**y**是一列有n个观测值的观测变量，或者直接说因变量以便于理解，

**X**是由多列特征组成的特征空间，假设有p列特征，简单理解就是有p个自变量，每个特征都有n个值，这与y是对应的：
$$
y=\begin{pmatrix}y_1\\y_2\\\vdots\\y_n\end{pmatrix}\quad,\quad X=\begin{pmatrix}X_1^T\\X_2^T\\\vdots\\X_n^T\end{pmatrix}=\begin{pmatrix}1&x_{11}&\cdots&x_{1p}\\1&x_{21}&\cdots&x_{2p}\\\vdots&\vdots&\ddots&\vdots\\1&x_{n1}&\cdots&x_{np}\end{pmatrix}
$$
$\beta$ 是系数向量，$\varepsilon$ 是干扰项（disturbance term）： 
$$
\beta=\begin{pmatrix}\beta_0\\\beta_1\\\beta_2\\\vdots\\\beta_p\end{pmatrix}\quad,\quad\varepsilon=\begin{pmatrix}\varepsilon_1\\\varepsilon_2\\\vdots\\\varepsilon_n\end{pmatrix}
$$
最后我们得到的第 $i$ 个 $y$（观测值）是这样的：
$$
y_i=\beta_01+\beta_1x_{i1}+\cdots+\beta_px_{ip}+\varepsilon_i\left(i=1,\cdots,n,\beta_0\text{为截距 }\right)
$$
所以，线性回归的公式是这样子的：
$$
y=X\beta+\varepsilon 
$$
前面说过，线性回归的过程就是要**找到最优的模型来描述数据**。这里就产生了两个问题：

- 如何定义“最优”？
- 如何寻找“最优”？

想要评价一个模型的优良，就需要一个**度量标准**。对于回归问题，最常用的度量标准就是均方差**MSE**，均方差是指预测值和实际值之间的平均方差。平均方差越小，说明测试值和实际值之间的差距越小，即模型性能更优。

在线性回归的式子中 $y$ 和 $X$ 是给定的，而β和ε是不确定的，也就是说，<mark>找到最优的$\beta$ 和 $\varepsilon$，就找到了最优的模型</mark>。

综合以上结论，可以用如下式子描述：
$$
(\beta^*,\varepsilon^*)=argmin\sum_{i=1}^m(f(x_i)-y_i)^2
$$
其中，$\beta^*$ 和 $\varepsilon^*$ 是要求的最优参数，右部是最小化均方差。

确了我们的目标，接下来就是该如何去寻找这两个量呢？最常用的是参数估计方法是 最小二乘法LSM , 最小二乘法试图找到一条直线，使得样本点和直线的欧氏距离之和最小。这个寻找的过程简单描述就是：根据凸函数的性质，求其关于$\beta$ 和 $\varepsilon$的二阶导的零点。

## 广义线性回归

上面我们得到了线性回归模型的数学原型，在数学上一个特例经常都是归属于一个更普遍或更一般的原型。让我们思考下面这两个回归模型： 
$$
y=\beta X+\varepsilon\quad,\quad lny=\beta X+\varepsilon 
$$
左边是我们之前得到的线性回归模型，右边是对数线性回归模型（log-Linear Regression）。从等式的形式来看，对数线性回归与线性回归区别仅仅在于等式左部，形式依旧是线性回归，但实质上是完成了输入空间 $X$ 到输出空间 $y$ 的**非线性映射**。这里的对数函数 $ln(·)$ ，将线性回归模型和真实观测**联系**起来。通俗地说，原本线性回归模型无法描述的非线性 $y$ ，套上了一个非线性函数 $ln(·)$ ，就可以描述对数形式的 $y$ 了。
$$
y=g^{-1}(\beta X+\varepsilon)
$$
将以上两个式子综合，写成更一般的形式就是广义线性回归模型（GLM，Generalized Linear Model）了。这里的 $g(·) $，即 $ln(·)$ ，是一个单调可微函数，称为**联系函数（Link Function）**。显然，前面的线性回归和对数回归都是广义线性回归的特例，根据联系函数的不同，以不同的方式映射，可以是对数，可以是指数，也可以是其他更复杂的函数。

## **逻辑回归**

经过上面的铺垫，终于可以愉快地谈谈逻辑回归了✪ ω ✪

当我们想将线性回归应用到**分类问题**中该怎么办呢？比如二分类问题，将 $X$ 对应的 $y$ 分为类别0和类别1。我们知道，线性回归本身的输出是连续的，也就是说要将连续的值分为离散的0和1。答案很容易想到，找到一个联系函数，将 $X$ 映射到 $y∈\{0，1\}$。

可能大家立马会想到单位阶跃函数（unit-step Function），函数原型如下：
$$
\left.y=\left\{\begin{array}{ll}0,&z<0\\0.5,&z=0\\1,&z>0\end{array}\right.\right.
$$
单位阶跃函数的确直接明了，小于0为类别0，大于0为类别1，等于0则皆可。但是有一个原则性的问题，我们需要的联系函数，必须是一个单调可微的函数，也就是说必须是连续的。（关于连续和可微的概念，忘了的同学赶紧回去补高数吧╮（╯＿╰）╭）。

这里写给出一个结论，逻辑回归使用的联系函数是Sigmoid函数（S形函数）中的最佳代表，即对数几率函数（Logistic Function），函数原型如下： 
$$
y=\frac1{(1+e^{-z})}
$$
为什么叫对数几率呢，因为它本来是长这样子的：
$$
ln\frac y{1-y}=z
$$
式子中的 $\frac{y}{1-y}$ 就是所谓的几率（odds）

将对数几率函数代入到之前得到的广义线性回归模型中，就可以得到逻辑回归的数学原型了：
$$
y=\frac1{1+e^{-(\beta X+\varepsilon)}}	
$$
**( •̀ ω •́ )✧所以逻辑回归也是广义线性回归中的一种以对数几率函数为联系函数的特例。**

接下来就是找到最优参数。

与线性回归不同的是，逻辑回归由于其联系函数的选择，它的参数估计方法不再使用最小二乘法，而是极大似然法。

最小二乘法是**最小化预测和实际之间的欧氏距离**，极大似然法的思想也是如出一辙的，但是它是通过**最大化预测属于实际的概率**来最小化预测和实际之间的“距离”。详细推导涉及凸优化理论，梯度下降法，牛顿法等，就不展开了。

## 线性回归和逻辑回归

- 线性回归和逻辑回归都是**广义线性回归模型的特例**
- 线性回归只能用于**回归问题**，逻辑回归用于**分类问题**（可由二分类推广至多分类）
- 线性回归无联系函数或不起作用，逻辑回归的联系函数是**对数几率函数**，属于Sigmoid函数
- 线性回归使用**最小二乘法**作为参数估计方法，逻辑回归使用**极大似然法**作为参数估计方法