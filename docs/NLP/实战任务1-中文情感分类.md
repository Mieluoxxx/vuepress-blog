---
title: 实战任务1：中文情感分类
---
## 实战任务：中文情感二分类问题
### 加载编码工具
```python
from transformers import BertTokenizer
token = BertTokenizer.from_pretrained('bert-base-chinese')
token
```

### 试编码句子
```python
out = token.batch_encode_plus(
	batch_text_or_text_pairs=['从明天起，做一个幸福的人。', '喂马，劈柴，周游世界。'],
	truncation=True,
	padding='max_length',
	max_length=17,
	return_tensors='pt',
	return_length=True)
# 查看编码输出
for k, v in out.items():
	print(k, v.shape)
	# 把编码还原为句子
	print(token.decode(out['input_ids'][0]))
```

### 定义数据集
用的还是 ChnSentiCorp数据集，是一个中文情感分析数据集，包含酒店、笔记本电脑和书籍的网购评论

```python
import torch
from datasets import load_dataset
class Dataset(torch.utils.data.Dataset):
	def __init__(self, split):
		self.dataset = load_dataset(path='lansinuote/ChnSentiCorp')[split]
	def __len__(self):
		return len(self.dataset)
	def __getitem__(self, i):
		text = self.dataset[i]['text']
		label = self.dataset[i]['label']
		return text, label
		
dataset = Dataset('train')
len(dataset), dataset[20]
```

### 设备选择
```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```

### 数据整理函数
```python
# 数据整理函数
def collate_fn(data):
	sents = [i[0] for i in data]
	labels = [i[1] for i in data]
	# 编码
	data = token.batch_encode_plus(
		batch_text_or_text_pairs=sents,
		truncation=True,
		padding='max_length',
		max_length=500,
		return_tensors='pt',
		return_length=True)
	# input_ids：编码之后的数字
	# attention_mask：补零的位置是0, 其他位置是1
	input_ids = data['input_ids']
	attention_mask = data['attention_mask']
	token_type_ids = data['token_type_ids']
	labels = torch.LongTensor(labels)
	# 把数据移动到计算设备上
	input_ids = input_ids.to(device)
	attention_mask = attention_mask.to(device)
	token_type_ids = token_type_ids.to(device)
	labels = labels.to(device)
	return input_ids, attention_mask, token_type_ids, labels
```

### 模拟一批数据
```python
# 模拟一批数据
data = [
	('你站在桥上看风景', 1),
	('看风景的人在楼上看你', 0),
	('明月装饰了你的窗子', 1),
	('你装饰了别人的梦', 0),]

# 试算
input_ids, attention_mask, token_type_ids, labels = collate_fn(data)
input_ids.shape, attention_mask.shape, token_type_ids.shape, labels
```

### 数据集加载器
```python
# 数据集加载器
loader = torch.utils.data.DataLoader(dataset=dataset,
	batch_size=16,
	collate_fn=collate_fn,
	shuffle=True,
	drop_last=True)
len(loader)
```

### 查看数据样例
```python
# 查看数据样例
for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):
	break
input_ids.shape, attention_mask.shape, token_type_ids.shape, labels
```

### 加载与训练数据集
```python
# 加载与训练数据集
from transformers import BertModel
pretrained = BertModel.from_pretrained('bert-base-chinese')
# 统计参数量
sum(i.numel() for i in pretrained.parameters()) / 10000
```

### 不训练预训练模型，不需要计算梯度
```python
# 不训练预训练模型，不需要计算梯度
for param in pretrained.parameters(): 
	param.requires_grad_(False)
```

### 预训练模型试算
```python
# 预训练模型试算
# 设定计算设备
pretrained.to(device)
# 模型试算
out = pretrained(input_ids=input_ids,
attention_mask=attention_mask,
token_type_ids=token_type_ids)
out.last_hidden_state.shape
```

### 定义下游模型

![1700906040142.png](http://pic.moguw.top/i/2023/11/25/6561c43bd7d22.png)

**NLL损失 NLLLoss（Negative Log Likelihood Loss）：**
![1700906062424.png](http://pic.moguw.top/i/2023/11/25/6561c45007416.png)

**交叉熵损失 torch.nn.CrossEntropyLoss()：**CrossEntropyLoss 等价于 LogSoftmax + NLLLoss
![1700906081115.png](http://pic.moguw.top/i/2023/11/25/6561c4622f98e.png)
```python
# 定义下游任务模型
class Model(torch.nn.Module):
	def __init__(self):
		super().__init__()
		self.fc = torch.nn.Linear(768, 2)
	def forward(self, input_ids, attention_mask, token_type_ids):
		# 使用预训练模型抽取数据特征
		with torch.no_grad():
			out = pretrained(input_ids=input_ids,
			attention_mask=attention_mask,
			token_type_ids=token_type_ids)
		# 对抽取的特征只取第1个字的结果做分类即可
		out = self.fc(out.last_hidden_state[:, 0])
		out = out.softmax(dim=1)
	return out

model = Model()
# 设定计算设备
model.to(device)
# 试算
model(input_ids=input_ids,
	attention_mask=attention_mask,
	token_type_ids=token_type_ids).shape
```

### 训练模型
```python
from transformers import AdamW
from transformers.optimization import get_scheduler

def train():
	# 定义优化器
	optimizer = AdamW(model.parameters(), lr=5e-4)
	# 定义loss函数
	criterion = torch.nn.CrossEntropyLoss()
	# 定义学习率调节器
	scheduler = get_scheduler(name='linear',
		num_warmup_steps=0,
		num_training_steps=len(loader),
		optimizer=optimizer)

	# 将模型切换到训练模式
	model.train()
	# 按批次遍历训练集中的数据
	for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):
		# 模型计算
		out = model(input_ids=input_ids,
			attention_mask=attention_mask,
			token_type_ids=token_type_ids)
	# 计算loss并使用梯度下降优化模型参数
	loss = criterion(out, labels)
	loss.backward()
	optimizer.step()
	scheduler.step()
	optimizer.zero_grad()
	# 输出各项数据的情况,便于观察
	if i % 10 == 0:
		out = out.argmax(dim=1)
		accuracy = (out==labels).sum().item() / len(labels)
		lr = optimizer.state_dict()['param_groups'][0]['lr']
		print(i, loss.item(), lr, accuracy)
		
train()
```

### 测试模型
```python
# 测试
def test():
	# 定义测试数据集加载器
	loader_test = torch.utils.data.DataLoader(dataset=Dataset('test'),
	batch_size=32,
	collate_fn=collate_fn,
	shuffle=True,
	drop_last=True)
	# 将下游任务模型切换到运行模式
	model.eval()
	correct = 0
	total = 0
	# 按批次遍历测试集中的数据
	for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):
		# 计算5个批次即可,不需要完全遍历
		if i == 5:
			break
		print(i)

		# 计算
		with torch.no_grad():
			out = model(input_ids=input_ids,
			attention_mask=attention_mask,
			token_type_ids=token_type_ids)

		# 统计正确率
		out = out.argmax(dim=1)
		correct += (out==labels).sum().item()
		total += len(labels)
	print(correct / total)

test()
```